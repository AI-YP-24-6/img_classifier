{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff9fbee-4889-4c92-b843-51347d606640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/milia/miniforge3/envs/rapids-24.10/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import random\n",
    "import logging\n",
    "import gdown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "    DRIVE_DIR = os.path.join(\"/content/drive\", \"MyDrive\")\n",
    "except ImportError:\n",
    "    DRIVE_DIR = os.getcwd()\n",
    "\n",
    "\n",
    "DATASET_DIR = os.path.join(os.getcwd(), \"dataset\")\n",
    "TEMP_DIR = os.path.join(os.getcwd(), \"temp\")\n",
    "ZIP_PATH = os.path.join(DRIVE_DIR, \"dataset_32_classes.zip\")\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "\n",
    "# добавляем главную директорию в путь\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "from Tools import find_image_files\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "pd.options.display.float_format = \"{:.4f}\".format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62ff1075-5c4e-47ca-9fed-4bc5e06f0431",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = \"1FKZ9oHZ3zFMoFJX2f2aI34M2XZ2ikSb0\"\n",
    "if os.path.exists(ZIP_PATH):\n",
    "    print(\"Архив уже добавлен\")\n",
    "else:\n",
    "    gdown.download(\n",
    "        f\"https://drive.google.com/uc?id={file_id}\",\n",
    "        os.path.join(os.getcwd(), \"dataset_32_classes.zip\"),\n",
    "        quiet=False,\n",
    "    )\n",
    "\n",
    "# Распаковка архива\n",
    "with zipfile.ZipFile(ZIP_PATH, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./dataset\")\n",
    "\n",
    "classes = os.listdir(DATASET_DIR)\n",
    "\n",
    "# Проверим структуру папок\n",
    "assert len(classes) == 32\n",
    "print(f\"Количество папок: {len(classes)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8363ed9b-44e3-461f-b285-7c615ec68580",
   "metadata": {},
   "source": [
    "from cuml.svm import SVC, SVR, LinearSVC\n",
    "import torch\n",
    "from rmm.allocators.torch import rmm_torch_allocator\n",
    "import rmm\n",
    "\n",
    "rmm.mr.set_current_device_resource(rmm.mr.CudaMemoryResource())\n",
    "torch.cuda.memory.change_current_allocator(rmm_torch_allocator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43a3abbc-45da-4f75-9aac-1ebae060613e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (1): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.device_count())\n",
    "    print(torch.cuda.current_device())\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vgg16 = models.vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "if torch.cuda.is_available():\n",
    "    vgg16 = vgg16.cuda()\n",
    "\n",
    "# Убираем последний слой\n",
    "feature_extractor = nn.Sequential(*list(vgg16.children())[:-1])\n",
    "feature_extractor.cuda()\n",
    "feature_extractor.eval()  # Установить в режим оценки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c393a2f2-ad0c-4de3-b7bb-a2a53879baca",
   "metadata": {},
   "source": [
    "Предварительная обработка для VGG16 (или ResNet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e345aaef-a709-4208-8a79-923bfe784ad6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        # transforms.Resize((224, 224)),          # Изменить размер изображения\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),  # Преобразовать изображение в тензор\n",
    "        transforms.Normalize(\n",
    "            mean=[\n",
    "                0.485,\n",
    "                0.456,\n",
    "                0.406,\n",
    "            ],  # Нормализация с использованием статистики над датасетом ImageNet\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733f5b11-9797-47a4-9417-8002eb120687",
   "metadata": {},
   "source": [
    "параметры Normalize были взяты из документации [pytorch](https://pytorch.org/vision/stable/transforms.html#transforming-and-augmenting-images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3038d7ce-6c9b-435f-b18d-8bc6ddc53143",
   "metadata": {},
   "source": [
    "Функция предварительной обработки одного изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e7d69a7-f9f3-4f6a-b857-65fcfe68a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"error {e} for {path}\")\n",
    "        # image = Image.open(r\".\\dataset\\Apple\\10_10_100.jpg\").convert(\"RGB\")\n",
    "    return preprocess(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c0ab41-9273-418a-a1e7-b00c7eba3b25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T02:26:27.728599Z",
     "iopub.status.busy": "2024-12-15T02:26:27.728599Z",
     "iopub.status.idle": "2024-12-15T02:26:27.733276Z",
     "shell.execute_reply": "2024-12-15T02:26:27.732755Z",
     "shell.execute_reply.started": "2024-12-15T02:26:27.728599Z"
    }
   },
   "source": [
    "Получение по N изображений каждого класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3d7672e-a4d1-45d6-a6cb-729bd3c0dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_images(number, directory):\n",
    "    return_images = []\n",
    "    return_class = []\n",
    "    for image_class in os.listdir(directory):\n",
    "        temp_images = find_image_files(directory + \"/\" + image_class)\n",
    "        return_images += temp_images[:number]\n",
    "        return_class += [image_class] * number\n",
    "    return return_images, return_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663ce002-ee40-42e9-a312-fb1bfdb94676",
   "metadata": {},
   "source": [
    "Функция извлечения и вытаскивания параметров из изображений.\n",
    "\n",
    "Для ускорения процесса испольузем вычисления на видеокарте, что сокращает время обработки с 1 часа до 4 минут при размерах изображения 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a3fa760-2d04-45f4-bad6-d8754b5208e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_dataset(image_paths: str | list[str], feature_extractor=feature_extractor):\n",
    "    all_features = None\n",
    "    if isinstance(image_paths, str):\n",
    "        image_paths = [image_paths]\n",
    "    for path in image_paths:\n",
    "        with torch.no_grad():\n",
    "            input_tensor = preprocess_image(path)\n",
    "            features = feature_extractor(input_tensor.cuda()).cuda()\n",
    "            try:\n",
    "                all_features = torch.cat((all_features, features.view(features.size(0), -1)), dim=0)\n",
    "            except TypeError:\n",
    "                all_features = features.view(features.size(0), -1)\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f1d10e-7f0b-4901-859d-436ba676d895",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T02:30:34.768021Z",
     "iopub.status.busy": "2024-12-15T02:30:34.768021Z",
     "iopub.status.idle": "2024-12-15T02:30:34.772020Z",
     "shell.execute_reply": "2024-12-15T02:30:34.772020Z",
     "shell.execute_reply.started": "2024-12-15T02:30:34.768021Z"
    }
   },
   "source": [
    "похоже из-за доступного кеша, пока tensor не разросся он реально имеет буст при обработке на видеокатре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "393d0eb8-c29d-4784-a1b3-376c55025933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def multi_extract_features_from_dataset(ndarray, names):\n",
    "#     ans = []\n",
    "#     t = 0\n",
    "#     for i in range(ndarray, 0 , 400):\n",
    "#         print(i)\n",
    "#         ans += map(extract_features_from_dataset, ndarray[t:i+1])\n",
    "#         t+=i\n",
    "#     return ans, names\n",
    "\n",
    "\n",
    "def multi_extract_features_from_dataset(\n",
    "    ndarray, names\n",
    "):  # похоже из-за доступного кеша, пока tensor  не разросся он реально имеет буст какой-то\n",
    "    return list(map(extract_features_from_dataset, ndarray)), names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3af42593-7ce4-49dd-8873-a44b5043dfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   1 tasks      | elapsed:   39.5s\n",
      "[Parallel(n_jobs=12)]: Done   3 out of  12 | elapsed:   41.5s remaining:  2.1min\n",
      "[Parallel(n_jobs=12)]: Done   5 out of  12 | elapsed:   42.3s remaining:   59.2s\n",
      "[Parallel(n_jobs=12)]: Done   7 out of  12 | elapsed:   43.1s remaining:   30.8s\n",
      "[Parallel(n_jobs=12)]: Done   9 out of  12 | elapsed:   43.8s remaining:   14.6s\n",
      "[Parallel(n_jobs=12)]: Done  12 out of  12 | elapsed:   44.8s finished\n"
     ]
    }
   ],
   "source": [
    "image_number = 600\n",
    "image_paths, labels = get_number_images(image_number, DATASET_DIR)\n",
    "# image_paths = [item for item in zip(image_paths, labels)]\n",
    "# dataset_features = extract_features_from_dataset(image_paths, feature_extractor)\n",
    "# print(\"Dataset features shape:\", dataset_features.shape)\n",
    "\n",
    "N_CORES = 12  # количество задействованных ядер процессора\n",
    "\n",
    "list_array = np.array_split(image_paths, N_CORES)\n",
    "labels_array = np.array_split(labels, N_CORES)\n",
    "data = Parallel(n_jobs=N_CORES, verbose=11)(\n",
    "    delayed(multi_extract_features_from_dataset)(array, names) for array, names in zip(list_array, labels_array)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdcfdb37-329d-4c00-b58b-b989ebfc0905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7423518-c7c5-4278-b040-3960fc66411d",
   "metadata": {},
   "source": [
    "собираем тензоры в удобный вид"
   ]
  },
  {
   "cell_type": "raw",
   "id": "32fceee2-d100-490d-a95d-bdca85c7e735",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "df = pd.DataFrame()\n",
    "names_df = pd.DataFrame()\n",
    "for job in data:\n",
    "    class_names = job[1]\n",
    "    names_df = pd.concat([names_df, pd.DataFrame(data=class_names, index=None, columns=None)], ignore_index=True)\n",
    "\n",
    "\n",
    "data = torch.cat(tuple(new_tensor for jobs in data for new_tensor in jobs[0]) , dim=0) # однострочник считающий все тензоры в 1 строку\n",
    "df = pd.DataFrame(data.cpu())\n",
    "\n",
    "df.to_csv(\"./tensors.csv\", encoding=\"utf-8-sig\", index=False,)\n",
    "names_df.to_csv(\"./names_df.csv\", encoding=\"utf-8-sig\", index=False,)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9912bdec-db33-4eda-b54e-2c5bfc30a4d6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "names_df = pd.read_csv(\"./names_df.csv\", encoding=\"utf-8-sig\")\n",
    "names_df = names_df.rename(columns={'0': 0})"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85d8e9a7-ca64-499a-a4d0-208da0f4be23",
   "metadata": {},
   "source": [
    "# df = pd.read_csv(\"./tensors.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# быстрый способ, если столбцов очень много\n",
    "txt = open(\"./tensors.csv\").readlines()\n",
    "df = pd.DataFrame(columns=[i for i in range(len(txt))])\n",
    "txt = open(\"./tensors.csv\").readlines()\n",
    "for i, ln in enumerate(txt):\n",
    "  row_items = ln.split()\n",
    "  df[i] = row_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3209b58-4a67-41b8-a913-da807aa05212",
   "metadata": {},
   "source": [
    "расчет на видеокарте с помощью RAPIDS"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b07ed5a-579f-442b-b865-b5e7e9f39d4c",
   "metadata": {},
   "source": [
    "import rmm\n",
    "import cupy\n",
    "pool = rmm.mr.PoolMemoryResource(rmm.mr.CudaMemoryResource(), initial_pool_size=2**28, maximum_pool_size=2**32)\n",
    "rmm.mr.set_current_device_resource(pool)\n",
    "cupy.cuda.set_allocator(rmm.allocators.cupy.rmm_cupy_allocator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2151d68-09f5-487a-bd8f-b1696d7fd154",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W] [23:03:17.708637] QWL-QN: max iterations reached\n",
      "[W] [23:03:17.708835] Maximum iterations reached before solver is converged. To increase model accuracy you can increase the number of iterations (max_iter) or improve the scaling of the input data.\n",
      "[W] [23:03:30.706130] QWL-QN: max iterations reached\n",
      "[W] [23:03:30.706305] Maximum iterations reached before solver is converged. To increase model accuracy you can increase the number of iterations (max_iter) or improve the scaling of the input data.\n",
      "[W] [23:03:44.134167] QWL-QN: max iterations reached\n",
      "[W] [23:03:44.134334] Maximum iterations reached before solver is converged. To increase model accuracy you can increase the number of iterations (max_iter) or improve the scaling of the input data.\n",
      "[W] [23:04:27.473269] QWL-QN: max iterations reached\n",
      "[W] [23:04:27.473447] Maximum iterations reached before solver is converged. To increase model accuracy you can increase the number of iterations (max_iter) or improve the scaling of the input data.\n",
      "[W] [23:04:56.819098] QWL-QN: max iterations reached\n",
      "[W] [23:04:56.819264] Maximum iterations reached before solver is converged. To increase model accuracy you can increase the number of iterations (max_iter) or improve the scaling of the input data.\n",
      "[W] [23:05:24.816835] QWL-QN: max iterations reached\n",
      "[W] [23:05:24.817002] Maximum iterations reached before solver is converged. To increase model accuracy you can increase the number of iterations (max_iter) or improve the scaling of the input data.\n",
      "[W] [23:05:38.143960] QWL-QN: max iterations reached\n",
      "[W] [23:05:38.144118] Maximum iterations reached before solver is converged. To increase model accuracy you can increase the number of iterations (max_iter) or improve the scaling of the input data.\n",
      "[W] [23:05:52.290765] QWL-QN: max iterations reached\n",
      "[W] [23:05:52.290937] Maximum iterations reached before solver is converged. To increase model accuracy you can increase the number of iterations (max_iter) or improve the scaling of the input data.\n",
      "[W] [23:06:05.796730] QWL-QN: max iterations reached\n",
      "[W] [23:06:05.796920] Maximum iterations reached before solver is converged. To increase model accuracy you can increase the number of iterations (max_iter) or improve the scaling of the input data.\n",
      "[W] [23:06:17.617096] QWL-QN: max iterations reached\n",
      "[W] [23:06:17.617269] Maximum iterations reached before solver is converged. To increase model accuracy you can increase the number of iterations (max_iter) or improve the scaling of the input data.\n",
      "[W] [23:06:44.139395] QWL-QN: max iterations reached\n",
      "[W] [23:06:44.139573] Maximum iterations reached before solver is converged. To increase model accuracy you can increase the number of iterations (max_iter) or improve the scaling of the input data.\n",
      "[W] [23:06:57.366305] QWL-QN: max iterations reached\n",
      "[W] [23:06:57.366470] Maximum iterations reached before solver is converged. To increase model accuracy you can increase the number of iterations (max_iter) or improve the scaling of the input data.\n",
      "[W] [23:07:10.267341] QWL-QN: max iterations reached\n",
      "[W] [23:07:10.267520] Maximum iterations reached before solver is converged. To increase model accuracy you can increase the number of iterations (max_iter) or improve the scaling of the input data.\n",
      "[W] [23:07:35.924690] QWL-QN: max iterations reached\n",
      "[W] [23:07:35.924862] Maximum iterations reached before solver is converged. To increase model accuracy you can increase the number of iterations (max_iter) or improve the scaling of the input data.\n",
      "[W] [23:08:21.357464] QWL-QN: max iterations reached\n",
      "[W] [23:08:21.357630] Maximum iterations reached before solver is converged. To increase model accuracy you can increase the number of iterations (max_iter) or improve the scaling of the input data.\n",
      "[W] [23:08:34.489716] QWL-QN: max iterations reached\n",
      "[W] [23:08:34.489890] Maximum iterations reached before solver is converged. To increase model accuracy you can increase the number of iterations (max_iter) or improve the scaling of the input data.\n",
      "[W] [23:09:11.588510] QWL-QN: max iterations reached\n",
      "[W] [23:09:11.588672] Maximum iterations reached before solver is converged. To increase model accuracy you can increase the number of iterations (max_iter) or improve the scaling of the input data.\n",
      "[W] [23:09:25.574546] QWL-QN: max iterations reached\n",
      "[W] [23:09:25.574718] Maximum iterations reached before solver is converged. To increase model accuracy you can increase the number of iterations (max_iter) or improve the scaling of the input data.\n",
      "SVM test accuracy: 0.9669271111488342\n",
      "SVM train accuracy: 1.0\n",
      "Точность на тренировочных данных: 1.0000\n",
      "Матрица неточностей на тренировочных данных:\n",
      "[[361   0   0 ...   0   0   0]\n",
      " [  0 362   0 ...   0   0   0]\n",
      " [  0   0 377 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ... 354   0   0]\n",
      " [  0   0   0 ...   0 367   0]\n",
      " [  0   0   0 ...   0   0 375]]\n",
      "CPU times: user 6min 21s, sys: 1.49 s, total: 6min 23s\n",
      "Wall time: 6min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import cuml\n",
    "from cuml.svm import LinearSVC\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(names_df[0])\n",
    "\n",
    "# Split into training and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, encoded_labels, test_size=0.4, random_state=42)\n",
    "cuml.DBSCAN(max_mbytes_per_batch=3000)\n",
    "\n",
    "# Train an SVM\n",
    "# svm = SVC(kernel='rbf', probability=True, C=1.0, cache_size = 3000)\n",
    "\n",
    "svm = LinearSVC(loss=\"squared_hinge\", penalty=\"l1\", probability=True, C=1.0)\n",
    "svm.fit(X_train.to_numpy(), y_train)\n",
    "\n",
    "# Test the SVM\n",
    "accuracy = svm.score(X_test, y_test)\n",
    "print(\"SVM test accuracy:\", accuracy)\n",
    "\n",
    "accuracy = svm.score(X_train, y_train)\n",
    "print(\"SVM train accuracy:\", accuracy)\n",
    "\n",
    "# Прогнозирование на тренировочных данных\n",
    "y_pred_train = svm.predict(X_train)\n",
    "\n",
    "# Оценка точности на тренировочных данных\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "print(f\"Точность на тренировочных данных: {train_accuracy:.4f}\")\n",
    "\n",
    "# Матрица неточностей на тренировочных данных\n",
    "cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "print(\"Матрица неточностей на тренировочных данных:\")\n",
    "print(cm_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce02c204-3511-4dce-b9d6-b524efc7646b",
   "metadata": {},
   "source": [
    "SCV не удалось использовать т. к. в исходном коде есть алокация памяти, но нет ее освобождения.\n",
    "SVC в версии 24.12 и 25.02a еще не имеют исправления утечки памяти в реализации SVM [issue](https://github.com/rapidsai/cuml/pull/6073)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e980ea74-0d76-4b90-8bb0-ed0438c602d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(names_df[0])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, encoded_labels, test_size=0.4, random_state=42)\n",
    "cuml.DBSCAN(max_mbytes_per_batch=4000)\n",
    "# Train an SVM\n",
    "# svm = SVC(kernel='rbf', probability=True, C=1.0, cache_size = 3000)\n",
    "svm = LinearSVC(loss=\"squared_hinge\", penalty=\"l1\", probability=True, C=1.0)\n",
    "svm.fit(X_train.to_numpy(), y_train)\n",
    "\n",
    "# Test the SVM\n",
    "accuracy = svm.score(X_test, y_test)\n",
    "print(\"SVM test accuracy:\", accuracy)\n",
    "\n",
    "accuracy = svm.score(X_train, y_train)\n",
    "print(\"SVM train accuracy:\", accuracy)\n",
    "\n",
    "# Прогнозирование на тренировочных данных\n",
    "y_pred_train = svm.predict(X_train)\n",
    "\n",
    "# Оценка точности на тренировочных данных\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "print(f\"Точность на тренировочных данных: {train_accuracy:.4f}\")\n",
    "\n",
    "# Матрица неточностей на тренировочных данных\n",
    "cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "print(\"Матрица неточностей на тренировочных данных:\")\n",
    "print(cm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae60dcf0-e375-447c-84aa-cbb1172bf12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = \"LinearSVC_model.pickle\"\n",
    "pickle.dump(svm, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcec3ed8-83e1-4522-b1f1-813e69bf64fe",
   "metadata": {},
   "source": [
    "### Итоги"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaff512a-b8fd-416c-829e-338368a52e72",
   "metadata": {},
   "source": [
    "|Модель|Гиперпараметры|Размер изображения|Цветное|accuracy на трейне|accuracy на test|Время извлечения признаков датасета|Время обучения модели|\n",
    "|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|\n",
    "|LinearSVC GPU|C=1, loss='squared_hinge', penalty='l1'|128px|да|1.0|0.96|≈4 мин|≈6 мин|\n",
    "|LinearSVC GPU|C=1, loss='squared_hinge', penalty='l1'|128px|да|1.0|0.6|≈4 мин|≈6 мин|\n",
    "|SVC CPU|C=1, kernel='linear'|224px|да|1.0|0.96|≈4 мин|≈1.5 ч|\n",
    "|SVC GPU|C=1, kernel='linear'|224px|да|?|?|≈error|≈error|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc5fd8f-2c14-40c6-aab4-86f3e4905abe",
   "metadata": {},
   "source": [
    "Из-за нелинейно растущей (O(n^2)) сложности [SVM](https://scikit-learn.ru/stable/modules/svm.html#id10) желательно тренировать модель в google colab или на видеокарте"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aikernel",
   "language": "python",
   "name": "aikernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
